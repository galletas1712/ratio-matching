# config/config.yaml

defaults:
  - _self_

# -- Model Configuration --
model:
  name: "EleutherAI/pythia-2.8b"
  tokenizer_name_or_path: null
  trust_remote_code: true

  # For LoRA, target modules depend on the model architecture.
  # These are typical for Pythia/GPT-NeoX. Use print(model) to inspect your model's layers.
  lora_target_modules:
    - "query_key_value"
    - "dense"
    - "dense_h_to_4h"
    - "dense_4h_to_h"

# -- Dataset Configuration --
dataset:
  name: "claserken/hhrlhf-rejudged"
  sft_column: "chosen"
  eval_size_limit: 10
  train_size_limit: 10

# -- LoRA Configuration --
lora:
  enabled: true       # Set to true to enable LoRA, false for full fine-tuning
  r: 32               # LoRA rank
  lora_alpha: 32      # LoRA alpha scaling factor (often 2*r)
  lora_dropout: 0.05      # Dropout probability for LoRA layers
  bias: "none"        # Options: "none", "lora_only", "all"
  task_type: "CAUSAL_LM" # Should be CAUSAL_LM for SFT on decoder models

# -- Training Arguments --
training:
  output_dir: "./outputs"
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 8
  learning_rate: 1e-4 # LoRA often tolerates slightly higher LR than full fine-tuning (adjust as needed)
  num_train_epochs: 1
  eval_strategy: "steps"
  eval_steps: 1
  save_steps: 200
  logging_steps: 11
  warmup_steps: 150
  lr_scheduler_type: "cosine"
  optim: "adamw_torch" # Standard AdamW works well for LoRA, but RMSProp is more memory efficient
  report_to: "wandb"
  wandb_project: "sft-lora"
  run_name: "pythia-2.8b-sft-lora-debug"
  remove_unused_columns: false
  seed: 0
  
  do_initial_eval: false  # TODO: set to true to get baseline

# -- Hydra Run Configuration --
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}