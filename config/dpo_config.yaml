# config/dpo_config.yaml

defaults:
  - _self_

# -- Model Configuration --
model:
  name: "/root/dro/outputs/2025-07-02_08-34-24/epoch-0-step-119999" # Base model for DPO
  tokenizer: "EleutherAI/pythia-2.8b"
  # Optional: Specify a pre-trained SFT adapter directory to load and merge before DPO
  # sft_adapter_path: "/root/dro/wandb/run-20250430_055450-r8tmrihb/files/checkpoint-1884"

  # For LoRA, target modules depend on the model architecture.
  lora_target_modules:
    - "query_key_value"
    - "dense"
    - "dense_h_to_4h"
    - "dense_4h_to_h"

# -- Dataset Configuration --
dataset:
  name: "claserken/hhrlhf-rejudged" # Dataset with 'prompt', 'chosen', 'rejected' columns expected by DPOTrainer
  eval_size_limit: null
  train_size_limit: null

# -- LoRA Configuration (Optional, for LoRA DPO) --
lora:
  enabled: false       # Set to true to enable LoRA for DPO
  r: 16               # LoRA rank (can be same or different from SFT)
  lora_alpha: 32      # LoRA alpha scaling factor
  lora_dropout: 0.05  # Dropout probability for LoRA layers
  bias: "none"        # Options: "none", "lora_only", "all"
  task_type: "CAUSAL_LM"

# -- DPO Specific Configuration --
dpo:
  beta: 0.1           # DPO beta parameter (controls divergence from reference model)
  loss_type: "rebel" # DPO loss type ('sigmoid', 'hinge', 'ipo', 'kto_pair')

# -- Training Arguments --
training:
  output_dir: ???
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 256
  learning_rate: 1e-6 # DPO often requires a lower LR than SFT
  num_train_epochs: 3
  logging_strategy: "steps"
  logging_first_step: true
  eval_strategy: "steps"
  eval_steps: 100 # Evaluate less frequently than SFT potentially
  save_strategy: "steps"
  save_steps: 800
  logging_steps: 10
  warmup_steps: 100
  lr_scheduler_type: "linear"
  optim: "adamw_torch"
  # fp16: true # Use bf16 if available and supported by hardware
  bf16: true # Uncomment if using Ampere or newer GPUs
  report_to: "wandb"
  remove_unused_columns: false # Important for DPOTrainer
  seed: 0

wandb:
  wandb_project: "dpo-full"
  run_name: "pythia-2.8b-${dpo.loss_type}-${now:%Y-%m-%d-%H-%M-%S}"
  save_files: false # Set to true to save adapter checkpoints to wandb

# -- Hydra Run Configuration --
hydra:
  run:
    dir: outputs/dpo/${now:%Y-%m-%d}/${now:%H-%M-%S} # Separate output directory for DPO runs
