# config/config.yaml

defaults:
  - _self_

# -- Model Configuration --
model:
  name: "EleutherAI/pythia-2.8b"

  # For LoRA, target modules depend on the model architecture.
  # These are typical for Pythia/GPT-NeoX. Use print(model) to inspect your model's layers.
  lora_target_modules:
    - "query_key_value"
    - "dense"
    - "dense_h_to_4h"
    - "dense_4h_to_h"

# -- Dataset Configuration --
dataset:
  name: "claserken/hhrlhf-rejudged"
  sft_column: "chosen"
  eval_size_limit: null
  train_size_limit: null

# -- LoRA Configuration --
lora:
  enabled: true       # Set to true to enable LoRA, false for full fine-tuning
  r: 32               # LoRA rank
  lora_alpha: 32      # LoRA alpha scaling factor (often 2*r)
  lora_dropout: 0.05      # Dropout probability for LoRA layers
  bias: "none"        # Options: "none", "lora_only", "all"
  task_type: "CAUSAL_LM" # Should be CAUSAL_LM for SFT on decoder models

# -- Training Arguments --
training:
  output_dir: ???
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 32
  learning_rate: 5e-6 # LoRA often tolerates slightly higher LR than full fine-tuning (adjust as needed)
  num_train_epochs: 3
  logging_strategy: "steps"
  logging_first_step: true
  eval_strategy: "steps"
  eval_steps: 50
  save_strategy: "steps"
  save_steps: 200
  logging_steps: 10
  warmup_steps: 150
  lr_scheduler_type: "linear"
  optim: "adamw_torch" # Standard AdamW works well for LoRA, but RMSProp is more memory efficient
  fp16: true
  report_to: "wandb"
  remove_unused_columns: false
  seed: 0
  
wandb:
  wandb_project: "sft-lora"
  run_name: "pythia-2.8b-sft-lora-debug-${now:%Y-%m-%d-%H-%M-%S}"
  save_files: false

# -- Hydra Run Configuration --
hydra:
  run:
    dir: outputs/sft/${now:%Y-%m-%d}/${now:%H-%M-%S}