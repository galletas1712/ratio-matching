defaults:
  - _self_

# Model Configuration
model:
  name: "EleutherAI/pythia-70m"
  max_seq_len: 2048
  # For LoRA, target modules depend on the model architecture.
  lora_target_modules:
    - "query_key_value"
    - "dense"
    - "dense_h_to_4h"
    - "dense_4h_to_h"

# Dataset Configuration
dataset:
  name: "Anthropic/hh-rlhf"
  # For SFT, we train on the 'chosen' column, filtering out rows where it's empty/None
  split_logic: "train" # 'train' for chosen, can be adapted if needed

# LoRA Configuration
lora:
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# Training Arguments
training:
  output_dir: ??? # Managed by Hydra
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 256
  learning_rate: 5.0e-5
  num_train_epochs: 1
  max_grad_norm: 15.0
  logging_strategy: "steps"
  logging_first_step: true
  eval_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 800
  logging_steps: 10
  warmup_steps: 100
  lr_scheduler_type: "linear"
  optim: "adamw_torch"
  bf16: true
  report_to: "wandb"
  remove_unused_columns: false
  seed: 42

# W&B Configuration
wandb:
  project: "sft-full"
  run_name: "pythia-2.8b-sft-${now:%Y-%m-%d-%H-%M-%S}"
  save_files: false

# Hydra Run Configuration
hydra:
  run:
    dir: outputs/sft/${now:%Y-%m-%d}/${now:%H-%M-%S}
